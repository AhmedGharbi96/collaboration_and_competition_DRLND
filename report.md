# Learning Algorithm
In order to solve this environment, we used 2 [DDPG](https://arxiv.org/pdf/1509.02971.pdf) agents. The two agents interacts with the environment to generate trajectories,
and these trajectories are stored in a centralized replay buffer. When an agent samples from the replay buffer, it might sample experiences that were generated by the other agent.
So, in a way, one agent is not only learning to improve its policy, but also is learning about the other agent policy.
The notebook contains 2 main parts, in the first section we initialize the environment and explore the action space, observation space and information about
the environment. In the second part, we find the implementation. The second part is composed of:
1. Actor and critic models
2. The OUNoise and the replay buffer
3. The DDPG agent with a shared replay buffer
4. The training loop
# Neural Network Architectures
## Actor Neural Network
* Input layer having the size of the observation space
* A fully connected layer with 256 units
* A second fully connected layer with 128 units
* Output layer having the size of the action space

## Critic Neural Network
* Input layer having the size of the observation space
* A fully connected layer with 256 units
* A second fully connected layer with 128 units
* Output layer with 1 unit
# Hyperparameters
BUFFER_SIZE = int(1e5)  # replay buffer size
BATCH_SIZE = 256        # minibatch size
GAMMA = 0.99            # discount factor
TAU = 1e-3              # for soft update of target parameters
LR_ACTOR = 1e-4         # learning rate of the actor 
LR_CRITIC = 1e-4        # learning rate of the critic
WEIGHT_DECAY = 0        # L2 weight decay
update = 4              # number of updates per sampled batch



# Plots
The agent has successfully solved the environment after 178 episodes.
```
Episode 1100	Avg Score: 0.114	Score: 0.0955
Episode 1200	Avg Score: 0.168	Score: 0.1955
Episode 1300	Avg Score: 0.276	Score: 1.0455
Episode 1369	Avg Score: 0.515	Score: 2.5505
\Environment solved in 1369 episodes!

```
![average score over the last 100 episodes](https://github.com/AhmedGharbi96/collaboration_and_competition_DRLND/blob/main/tennis_env.png)

## Potential Improvements
In the future,I would like to implement [MADDPG](https://arxiv.org/abs/1706.02275) which is a multi-agent version of the DDPG agent, where all agents have a centrilized critic
that predicts the Q(X,a1,...,an) where X is a at least a concatenation of all the observations of the agents and a_i is the action taken
by agent i.
